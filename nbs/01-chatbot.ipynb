{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 02:08:44.271 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.273 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.276 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.279 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.279 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.282 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.285 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.288 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.290 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.292 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.293 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "Error fetching or processing , exception: Exactly one of file, filename and url must be specified.\n",
      "2024-10-05 02:08:44.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.302 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.316 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.317 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.319 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.321 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.321 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.323 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.327 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-05 02:08:44.327 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import pickle\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import tempfile\n",
    "import time\n",
    "import langchain\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "\n",
    "# Set your OpenAI API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "url_file_path = \"url_faiss_store_openai.pkl\"\n",
    "\n",
    "# Streamlit setup\n",
    "st.set_page_config(\n",
    "    page_title=\"Personal AI Assistant\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "st.markdown(\"<h1 style='text-align: center; color: black;'>ðŸ¤– Personal AI Assistant</h1>\", unsafe_allow_html=True)\n",
    "st.sidebar.markdown(\"<h3 style='text-align: center; color: black;'>Assistant Console</h3>\", unsafe_allow_html=True)\n",
    "\n",
    "# ---- URL Loading & Embedding ----\n",
    "num_links = st.sidebar.slider(\"How many links do you want to input?\", min_value=1, max_value=5, value=1)\n",
    "urls = [st.sidebar.text_input(f\"URL {i+1}\", key=f\"url{i}\") for i in range(num_links)]\n",
    "if urls:\n",
    "    loader = UnstructuredURLLoader(urls=urls)\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \".\"], chunk_size=1000)\n",
    "    url_docs = text_splitter.split_documents(data)\n",
    "    if url_docs:\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        url_vectorindex_openai = FAISS.from_documents(url_docs, embeddings)\n",
    "        with open(url_file_path, \"wb\") as f:\n",
    "            pickle.dump(url_vectorindex_openai, f)\n",
    "\n",
    "# ---- PDF Loading & Embedding ----\n",
    "uploaded_file = st.sidebar.file_uploader(\"Upload a PDF file\", type=['pdf'])\n",
    "if uploaded_file:\n",
    "    pdf_reader = PdfReader(uploaded_file)\n",
    "    pdf_text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        pdf_text += page.extract_text()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \".\"], chunk_size= 500)\n",
    "    pdf_docs = text_splitter.split_text(pdf_text)\n",
    "    if pdf_docs:\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        pdf_vectors = FAISS.from_texts(pdf_docs, embeddings)\n",
    "        \n",
    "\n",
    "# ---- Query Interface ----\n",
    "llm = OpenAI(temperature=0.9, max_tokens=500, openai_api_key=openai_api_key)\n",
    "data_source = st.selectbox(\"What do you want to inquire about?\", [\"URL\", \"PDF\"])\n",
    "\n",
    "if data_source == \"URL\":\n",
    "    query_url = st.text_input('Ask your question about URLs:')\n",
    "    if query_url:\n",
    "        if os.path.exists(url_file_path):  # Ensure URL database exists\n",
    "            with open(url_file_path, \"rb\") as f:\n",
    "                vectorstore = pickle.load(f)\n",
    "                chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())\n",
    "                result = chain({\"question\": query_url}, return_only_outputs=True)\n",
    "                st.header(\"Answer based on URLs:\")\n",
    "                st.subheader(result['answer'])\n",
    "\n",
    "elif data_source == \"PDF\":\n",
    "    query_pdf = st.text_input('Ask your question about PDFs:')\n",
    "    if query_pdf:\n",
    "        docs = pdf_vectors.similarity_search(query_pdf)\n",
    "\n",
    "        chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "        response = chain.run(input_documents=docs, question=query_pdf)\n",
    "           \n",
    "        st.write(response)\n",
    "\n",
    "    if st.button(\"Summarize PDF\"):\n",
    "        def summarize_pdfs_from_folder(pdfs_folder):\n",
    "            summaries = []\n",
    "            for pdf_file in pdfs_folder:\n",
    "                with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                    temp_path = temp_file.name\n",
    "                    temp_file.write(pdf_file.getvalue())\n",
    "                loader = PyPDFLoader(temp_path)\n",
    "                docs = loader.load_and_split()\n",
    "                chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "                summary = chain.run(docs)\n",
    "                summaries.append(summary)\n",
    "                os.remove(temp_path)\n",
    "            return summaries\n",
    "\n",
    "        summaries = summarize_pdfs_from_folder([uploaded_file])\n",
    "        for summary in summaries:\n",
    "            st.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.15.13-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Using cached lxml-5.3.0-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting nltk (from unstructured)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting tabulate (from unstructured)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured) (2.32.3)\n",
      "Collecting beautifulsoup4 (from unstructured)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 981.5/981.5 kB 9.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy<2 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.10.0-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured) (4.11.0)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.25.9-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Using cached wrapt-1.16.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured) (4.66.5)\n",
      "Requirement already satisfied: psutil in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured) (6.0.0)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->unstructured)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from dataclasses-json->unstructured) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk->unstructured)\n",
      "  Using cached regex-2024.9.11-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from requests->unstructured) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from requests->unstructured) (2024.8.30)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Collecting cryptography>=3.1 (from unstructured-client->unstructured)\n",
      "  Downloading cryptography-43.0.1-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
      "  Downloading deepdiff-8.0.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured-client->unstructured) (0.27.2)\n",
      "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: packaging>=23.1 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured-client->unstructured) (24.1)\n",
      "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
      "  Downloading pypdf-5.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting orderly-set==5.2.2 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
      "  Downloading orderly_set-5.2.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: anyio in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\anaconda\\envs\\openai-4o\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.0)\n",
      "Downloading unstructured-0.15.13-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.1 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------------------------------  2.1/2.1 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 586.9/586.9 kB 4.2 MB/s eta 0:00:00\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached lxml-5.3.0-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
      "Downloading rapidfuzz-3.10.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 17.1 MB/s eta 0:00:00\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading unstructured_client-0.25.9-py3-none-any.whl (45 kB)\n",
      "Using cached wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Downloading cryptography-43.0.1-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.0/3.1 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.0/3.1 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.0/3.1 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.0/3.1 MB 12.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.6/3.1 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 2.9/3.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading deepdiff-8.0.1-py3-none-any.whl (82 kB)\n",
      "Downloading orderly_set-5.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Downloading pypdf-5.0.1-py3-none-any.whl (294 kB)\n",
      "Using cached regex-2024.9.11-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=9e5f74aa11afb1c6b2caa3cfa6c295ee9492a2f207e017f40a734ae6135af49a\n",
      "  Stored in directory: c:\\users\\amit kamthane\\appdata\\local\\pip\\cache\\wheels\\95\\03\\7d\\59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "Successfully built langdetect\n",
      "Installing collected packages: filetype, wrapt, tabulate, soupsieve, regex, rapidfuzz, python-magic, python-iso639, pypdf, pycparser, orderly-set, olefile, lxml, langdetect, jsonpath-python, emoji, chardet, backoff, python-oxmsg, nltk, deepdiff, cffi, beautifulsoup4, cryptography, unstructured-client, unstructured\n",
      "Successfully installed backoff-2.2.1 beautifulsoup4-4.12.3 cffi-1.17.1 chardet-5.2.0 cryptography-43.0.1 deepdiff-8.0.1 emoji-2.14.0 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 lxml-5.3.0 nltk-3.9.1 olefile-0.47 orderly-set-5.2.2 pycparser-2.22 pypdf-5.0.1 python-iso639-2024.4.27 python-magic-0.4.27 python-oxmsg-0.0.1 rapidfuzz-3.10.0 regex-2024.9.11 soupsieve-2.6 tabulate-0.9.0 unstructured-0.15.13 unstructured-client-0.25.9 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-4o",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
